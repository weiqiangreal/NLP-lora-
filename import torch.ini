import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms
from torchvision.datasets import FakeData  # 用虚拟数据演示，可替换为真实数据集
import numpy as np

# ===================== 1. 定义LoRA核心层 =====================
class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=8, alpha=16):
        super().__init__()
        # LoRA低秩矩阵
        self.A = nn.Parameter(torch.randn(in_features, rank) / np.sqrt(rank))
        self.B = nn.Parameter(torch.zeros(rank, out_features))
        # 缩放因子
        self.scaling = alpha / rank
        # 冻结预训练权重，仅训练A和B
        self.weight = None  # 绑定主层的权重

    def forward(self, x):
        # LoRA输出 = x * A * B * scaling
        lora_out = x @ self.A @ self.B * self.scaling
        return lora_out

# ===================== 2. 封装带LoRA的线性层 =====================
class LinearWithLoRA(nn.Module):
    def __init__(self, linear_layer, rank=8, alpha=16):
        super().__init__()
        self.linear = linear_layer
        self.lora = LoRALayer(
            in_features=linear_layer.in_features,
            out_features=linear_layer.out_features,
            rank=rank,
            alpha=alpha
        )
        # 冻结原线性层权重
        for param in self.linear.parameters():
            param.requires_grad = False
        # 绑定LoRA层的weight属性（方便后续加载）
        self.lora.weight = self.linear.weight

    def forward(self, x):
        # 原线性层输出 + LoRA输出
        return self.linear(x) + self.lora(x)

# ===================== 3. 给预训练模型插入LoRA =====================
def add_lora_to_model(model, target_layers=["fc"], rank=8, alpha=16):
    """
    给模型的指定层添加LoRA
    :param model: 预训练模型
    :param target_layers: 需要插入LoRA的层名（如ResNet的fc层）
    :param rank: LoRA的秩（越小参数量越少，建议4-16）
    :param alpha: 缩放系数
    :return: 带LoRA的模型
    """
    for name, module in model.named_modules():
        if name in target_layers and isinstance(module, nn.Linear):
            # 替换为带LoRA的线性层
            setattr(model, name, LinearWithLoRA(module, rank, alpha))
    return model

# ===================== 4. 数据加载（示例用虚拟数据） =====================
def get_dataloader(batch_size=32):
    # 数据预处理
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    # 虚拟数据集（替换为你的真实数据集，如ImageFolder）
    train_dataset = FakeData(
        size=1000, image_size=(3, 224, 224), num_classes=10, transform=transform
    )
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    return train_loader

# ===================== 5. 训练主函数 =====================
def train_lora():
    # 设备配置
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"使用设备: {device}")

    # 1. 加载预训练模型（以ResNet18为例）
    model = models.resnet18(pretrained=True)
    # 给fc层添加LoRA（可扩展到更多层，如layer4的线性层）
    model = add_lora_to_model(model, target_layers=["fc"], rank=8, alpha=16)
    model = model.to(device)

    # 2. 只获取LoRA的可训练参数（大幅减少参数量）
    lora_params = [param for name, param in model.named_parameters() if "lora" in name]
    print(f"LoRA可训练参数数量: {sum(p.numel() for p in lora_params):,}")

    # 3. 优化器（仅优化LoRA参数）
    optimizer = optim.Adam(lora_params, lr=1e-4)
    criterion = nn.CrossEntropyLoss()

    # 4. 数据加载
    train_loader = get_dataloader(batch_size=32)

    # 5. 训练循环
    model.train()
    epochs = 5
    for epoch in range(epochs):
        total_loss = 0.0
        correct = 0
        total = 0
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)

            # 前向传播
            outputs = model(images)
            loss = criterion(outputs, labels)

            # 反向传播+优化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # 统计
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            # 打印批次信息
            if batch_idx % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}")

        # 打印epoch结果
        avg_loss = total_loss / len(train_loader)
        accuracy = 100 * correct / total
        print(f"Epoch [{epoch+1}/{epochs}] 完成 | 平均损失: {avg_loss:.4f} | 准确率: {accuracy:.2f}%")

    # 6. 保存LoRA权重（仅保存A和B矩阵，体积极小）
    lora_state_dict = {k: v for k, v in model.state_dict() if "lora" in k}
    torch.save(lora_state_dict, "lora_weights.pth")
    print("LoRA权重已保存到 lora_weights.pth")

    return model

# ===================== 6. 加载LoRA权重 =====================
def load_lora_weights(model, lora_path="lora_weights.pth"):
    lora_state_dict = torch.load(lora_path, map_location="cpu")
    model.load_state_dict(lora_state_dict, strict=False)
    print("LoRA权重加载完成")
    return model

# 运行训练
if __name__ == "__main__":
    trained_model = train_lora()
    # 加载权重示例
    # trained_model = load_lora_weights(trained_model)
#加油！